<?xml version="1.0" encoding="UTF-8"?>
<!-- 
	This is the system setup configuration file for the Social Network Connector.
	It configures low level startup parameter (like which crawlers to run) as well 
	as very low level system functionality.
	
	
	To configure the runtime behavior of the SNC, take a look at 
	* SNC_Runtime_Configuration.xml		- for general runtime configuration options
	* SNC_Crawler_Configuration/...		- for the constraints to the crawler
	* SNC_HANA_Configuration.xml		- for the hana configuration 
	* Administration-servlet.xml		- for the web administration servlet
	* web.xml							- for general web configuration
	* log4j.xml							- for logging options
	* quartz.properties					- for jmx support
	* properties/webparser.xml			- for available web parser that can be used by the web crawler
	
	The first thing you will want to configure, however is which configuration option
	for the crawler to use. Usually this is done via an xml file but you can also opt 
	for a database. If a file is used, then you need to define it in the section 
	configuration provider at around line 373 (well, was that line on September, the 
	17th anyway)


	Licensed to the Apache Software Foundation (ASF) under one or more contributor 
	license agreements. See the NOTICE file distributed with this work for additional 
	information regarding copyright ownership. The ASF licenses this file to 
	You under the Apache License, Version 2.0 (the "License"); you may not use 
	this file except in compliance with the License. You may obtain a copy of 
	the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required 
	by applicable law or agreed to in writing, software distributed under the 
	License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS 
	OF ANY KIND, either express or implied. See the License for the specific 
	language governing permissions and limitations under the License. 
-->
<!-- @version $Id: applicationContext.xml 561608 2007-08-01 00:33:12Z vgritsenko $ -->

<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
	xmlns:p="http://www.springframework.org/schema/p"
	xmlns:context="http://www.springframework.org/schema/context"
	xmlns:util="http://www.springframework.org/schema/util"
	xmlns:jdbc="http://www.springframework.org/schema/jdbc" 
	xmlns:tx="http://www.springframework.org/schema/tx"
	xsi:schemaLocation="http://www.springframework.org/schema/beans 
						http://www.springframework.org/schema/beans/spring-beans.xsd
						http://www.springframework.org/schema/context 
						http://www.springframework.org/schema/context/spring-context.xsd
						http://www.springframework.org/schema/jdbc 
						http://www.springframework.org/schema/jdbc/spring-jdbc.xsd
						http://www.springframework.org/schema/tx
						http://www.springframework.org/schema/tx/spring-tx.xsd
						http://www.springframework.org/schema/util 
						http://www.springframework.org/schema/util/spring-util.xsd">
                           
	<context:annotation-config />
		<context:component-scan base-package="de.comlineag.snc.appstate" />
		<context:component-scan base-package="de.comlineag.snc.constants" />
		<context:component-scan base-package="de.comlineag.snc.controller" />
		<context:component-scan base-package="de.comlineag.snc.crawler" />
		<context:component-scan base-package="de.comlineag.snc.crypto" />
		<context:component-scan base-package="de.comlineag.snc.data" />
		<context:component-scan base-package="de.comlineag.snc.handler" />
		<context:component-scan base-package="de.comlineag.snc.helper" />
		<context:component-scan base-package="de.comlineag.snc.neo4j" />
		<context:component-scan base-package="de.comlineag.snc.parser" />
		<context:component-scan base-package="de.comlineag.snc.persistence" />
	<tx:annotation-driven />
	
	<!-- Import Neo4j datasource configuration -->
	<!-- <import resource="spring-data.xml"/> -->
	
	<!-- sets the overall application context -->
	<bean id="contextApplicationContextProvider" class="de.comlineag.snc.appstate.ApplicationContextProvider"></bean>
	
	<!-- this is used to get the path to the WEB-INF directory from within the application without
		 an explicit implementation of the servlet context in each class that requires access to the directory.
		 Mainly it is implemented by a class ResourcePathHolder (instantiated by spring via this bean) that
		 holds a static variable and getter method to return the path to the WEB-INF directory -->
	<bean id="contextWebApplicationContextProvider" class="de.comlineag.snc.handler.ResourcePathHolder"></bean>
	
	
	<!-- this is to register a new MBeanServer for the JMX support - either use this or the automatic 
		 registration option below -->
<!-- 
	<bean id="mbeanServer" class="org.springframework.jmx.support.MBeanServerFactoryBean"/>
 -->
	<!-- this bean needs to be eagerly pre-instantiated in order for the exporting to occur;
		 this means that it must not be marked as lazily initialized -->
<!-- 
	<bean id="exporter" class="org.springframework.jmx.export.MBeanExporter">
		<property name="beans">
			<map>
				<entry key="bean:name=sncBean1" value-ref="sncBean"/>
			</map>
		</property>
		<property name="server" ref="mbeanServer"/>
	</bean>
	<bean id="sncBean" class="org.springframework.jmx.JmxTestBean">
		<property name="name" value="TEST"/>
		<property name="age" value="100"/>
	</bean>
-->
	
	<!-- This is for automatic registration with an existing MBeanServer for the JMX support 
		 - we need JMX for JWatch to monitor the SNC-->
	<!-- this bean must not be lazily initialized if the exporting is to happen -->
<!-- 
	<bean id="exporter" class="org.springframework.jmx.export.MBeanExporter" lazy-init="false">
		<property name="beans">
			<map>
				<entry key="bean:name=SimpleWebCrawlerBean" value-ref="SimpleWebCrawlerBean"/>
				<entry key="bean:name=TwitterCrawlerBean" value-ref="TwitterCrawlerBean"/>
				<entry key="bean:name=LithiumCrawlerBean" value-ref="LithiumCrawlerBean"/>
				<entry key="bean:name=HanaPersistenceBean" value-ref="HanaPersistenceBean"/>
			</map>
		</property>
	</bean>
	<bean id="SimpleWebCrawlerBean" class="de.comlineag.snc.crawler.SimpleWebCrawler">
		<property name="name" value="SimpleWebCrawler"/>
	</bean>
	<bean id="TwitterCrawlerBean" class="de.comlineag.snc.crawler.TwitterCrawler">
		<property name="name" value="TwitterCrawler"/>
	</bean>
	<bean id="LithiumCrawlerBean" class="de.comlineag.snc.crawler.LithiumCrawler">
		<property name="name" value="LithiumCrawler"/>
	</bean>
	<bean id="HanaPersistenceBean" class="de.comlineag.snc.persistence.HANAPersistence"></bean>
 -->
	
	
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
	     J O B    C O N T R O L    S E T T I N G S  -  J O B    C O N T R O L    S E T
	     this is the section in which to turn on/off the different crawler 
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	     <!-- Activate Spring annotation support -->
	<bean class="org.springframework.scheduling.quartz.SchedulerFactoryBean">
		<property name="autoStartup"><value>true</value></property>
		<property name="overwriteExistingJobs"><value>true</value></property>
		<property name="waitForJobsToCompleteOnShutdown"><value>false</value></property>

		<property name="quartzProperties">
			<props>
				<!-- ThreadPool -->
				<prop key="org.quartz.threadPool.class">org.quartz.simpl.SimpleThreadPool</prop>
				<prop key="org.quartz.threadPool.threadCount">10</prop>
			</props>
		</property>
		<property name="triggers">
			<list>
				<!-- SOCIAL NETWORK CRAWLER
					 activate each crawler you want the SNC to scan through. 
					 Make sure to set the threadCount number above according to the number of activated 
					 crawlers. The total number of threads must include the number of actual crawlers in
					 this section plus the FsCrawler below - resulting in number of crawlers plus 1  -->
				<ref bean="TwitterCrawlerTrigger" />
				<!-- <ref bean="FacebookCrawlerTrigger"/> -->
				<!-- <ref bean="GoogleCrawlerTrigger"/> -->
				<!-- <ref bean="LinkedinCrawlerTrigger"/> -->
				<!-- <ref bean="SimpleWebCrawlerTrigger"/> -->
				<ref bean="THEWebCrawlerTrigger"/>
				<ref bean="LithiumCrawlerTrigger" />
				
				
				<!-- FILESYSTEM CRAWLER - SAFEGUARD
					this is the fail save crawler. It scans through a backup directory (typically ./json)
					and retries to insert/update all posts and users (stored within there as json files)
					into the actual persistence manager (e.g. HANA or Neo4J. It is therefore a save guard
					for the runtime environment of the social network connector. 
					You should only deactivate it in case you do not save the posts and users of failed
					inserts in a backup directory (can be configured in SNC_Runtime_Configuration.xml), 
					in which case there would be no sense scanning :-) -->
				<!-- <ref bean="FsCrawlerTrigger" /> -->
			</list>
		</property>
	</bean>
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
	     END OF JOB CONTROL SECTION - END OF JOB CONTROL SECTION - END OF JOB CONTROL
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	
	
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
	     C R A W L E R    S E T T I N G S  -  C R A W L E R    S E T T I N G S  -  C R                                              
	     YOU NORMALLY DO NOT NEED TO ACTIVATE/DEACTIVATE A CRAWLER HERE, BUT YOU MAY 
		 NEED TO ADAPT KEYS AND THE LIKE. FOR EXAMPLE, TWITTER USES KEYS TO AUTHORIZE
		 THE CRAWLER (FACEBOOK ALSO) AND LITHIUM, ON THE OTHER HAND, MAY NEED TO GET THE
		 URL AND SERVER NAME ETC. ADAPTED.
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	
	<!-- T W I T T E R    C R A W L E R    (PRODUCTIVE)		-->
	<bean name="TwitterCrawlerDetail"
		class="org.springframework.scheduling.quartz.JobDetailFactoryBean">
		<property name="jobClass" value="de.comlineag.snc.crawler.TwitterCrawler" />
		<!-- <property name="concurrent" value="false" /> -->
		<property name="durability" value="true" />
		<property name="jobDataAsMap">
			<map>
				<!-- THIS MUST BE SET ACCORDING TO AN EXISTING CODE FOR A SOCIAL NETWORK ENTRY IN SNC_Runtime_Configuration.xml -->
				<entry key="SN_ID_CODE" value="TW" />
				
				<!-- These are the security tokens for the twitter API -->
				<entry key="consumerKey" value="zlGuAavBH2T23hIm57l8WA" />
				<entry key="consumerSecret" value="xzqH6lLWnXLvlHJooNDaCDXlzJzv976JcWt8y2eyHBk" />
				<entry key="token" value="754994-KDec8qJBwEggHwHQ9XO0X7QBx1VCOZwgbgtpYiibWjl" />
				<entry key="tokenSecret" value="qm1dqrJas8Lf2ANU8Lx470TkcUSndWLwvJ1I2huZKvJrc" />
				
				<entry key="connectionTimeout" value="1000" />
				
			</map>
		</property>
	</bean>
	<bean id="TwitterCrawlerTrigger"
		class="org.springframework.scheduling.quartz.SimpleTriggerFactoryBean">
		<property name="jobDetail" ref="TwitterCrawlerDetail" />
		<!-- 1 second delay after startup -->
		<property name="startDelay" value="1000" />
		<!-- repeat the job every 
							5000 - 50 seconds 
							6000 - 1 minutes 
							15000 - 2,5 minutes (round about) 
							30000 - 5 minutes -->
		<property name="repeatInterval" value="30000" />
	</bean>
	
	
	
	
	
	<!-- L I T H I U M    C R A W L E R    (PRODUCTIVE) -->
	<bean name="LithiumCrawlerDetail"
		class="org.springframework.scheduling.quartz.JobDetailFactoryBean">
		<property name="jobClass" value="de.comlineag.snc.crawler.LithiumCrawler" />
		<!-- <property name="concurrent" value="false" /> -->
		<property name="durability" value="true" />
		<property name="jobDataAsMap">
			<map>
				<!-- THIS MUST BE SET ACCORDING TO AN EXISTING CODE FOR A SOCIAL NETWORK ENTRY IN SNC_Runtime_Configuration.xml -->
				<entry key="SN_ID_CODE" value="CC" />
				
				<!-- These are security tokens - not used -->
				<entry key="consumerKey" value="" />
				<entry key="consumerSecret" value="" />
				<entry key="token" value="" />
				<entry key="tokenSecret" value="" />
				
				<!-- login settings - are encrypted using base64 -->
				<entry key="user" value="Y21yYWw=" />			<!-- cmral -->
				<entry key="passwd" value="MTIzVGVzdDEyMw==" />	<!-- 123Test123 -->
				
				<!-- with these entries the url to the communities' rest api is generated -->
				<entry key="protocol" value="https" />
				<entry key="port" value="443" />
				<entry key="server_url" value="wissen.cortalconsors.de" />
				<entry key="rest_api_loc" value="/restapi/vc" />
			</map>
		</property>
	</bean>
	<bean id="LithiumCrawlerTrigger"
		class="org.springframework.scheduling.quartz.SimpleTriggerFactoryBean">
		<property name="jobDetail" ref="LithiumCrawlerDetail" />
		<property name="startDelay" value="2000" />
		<property name="repeatInterval" value="86400000" /> <!-- 24 Stunden -->
	</bean>
	
	
	
	
	
	<!-- S I M P L E    W E B    C R A W L E R    (PRODUCTIVE) -->
	<!-- this is the easiest way of crawling a web page. It takes an initial url and some information (such as
		 how many links to follow, whether or not to follow links that lead off the initial domain or are not
		 below the initial given path and the like) and the crawler will fetch all relevant pages (a page is 
		 relevant if it contains any of the track terms from the crawler configuration file) that are linked 
		 to the initial url. It hands all pages to ParserControl for content extraction and passes the extracted
		 data to the persistence layer.
		 
		 Although the SimpleWebCrawler is, despite it's name, already quite sophisticated, it has some 
		 disadvantages:
		 
		 * First of all, you have to setup the initial URL to crawl within this file rather than in the crawler 
		   configuration file - that is a somewhat awkward design and difficult to configure. 
		 * Second you can only pass one webpage/domain as a starting point to crawl. 
		 * Third and finally, it is very cumbersome to use an individual SN_ID (social network) code for 
		   different sites. 
		 * Forth and last, the software design of the SimpleWebCrawler is single threaded and sequential
		   thus making it slow if it is to crawl a lot of pages
		 
		 In effect, if you want to crawl a complete site with a lot of pages and maybe even want to follow off 
		 of the initial site to other linked sites, you should opt for the THEWebCrawler below, as it is much 
		 faster in the desribed scenario. -->
	<bean name="SimpleWebCrawlerDetail"
		class="org.springframework.scheduling.quartz.JobDetailFactoryBean">
		<property name="jobClass" value="de.comlineag.snc.crawler.SimpleWebCrawler" />
		<property name="durability" value="true" />
		<property name="jobDataAsMap">
			<map>
				<!-- THIS MUST BE SET ACCORDING TO AN EXISTING CODE FOR A SOCIAL NETWORK ENTRY IN SNC_Runtime_Configuration.xml -->
				<entry key="SN_ID_CODE" value="WO" />
				
				<!-- set the starting url - the SimpleWebCrawler can only process ONE starting url 
					 if you need/want to setup multiple parallel crawlers for different sites you
					 can either duplicate this entire bean section plus the trigger or use the 
					 MultiWebCrawler below. -->
				<entry key="server_url" value="http://www.wallstreet-online.de" />
								
				<!-- if the website requires authentication, set username and password here -->
				<entry key="user" value="Y29taW5lMjAxNA==" />		<!-- comine2014 -->
				<entry key="passwd" value="TUd1LTZ0Yy1BUjUtdTdS" /> <!-- MGu-6tc-AR5-u7R -->
				
				<!-- max_depth - how many levels of cascaded links to follow. 
					 Be careful what you do here. A level of three would mean: follow all links from the page 
					 that was linked from the page that was linked from the page that was linked from the 
					 starting url.
					  
					 If you set this higher then the value of WC_MAX_DEPTH in SNC_Runtim_Configuration (5 at
					 the moment) then that value will be used. This Runtime configuration acts as a safety 
					 margin to not exceed a certain scanning depth. 
					 You can, however, also set this to -1 in which case there will be no limit on how deep 
					 links will be followed Be advised though, as this may lead to quite a huge amount of pages. 
					 As a last safeguard in case this value is set to  -1, this will also implicitly set the 
					 parameter of stayOnDomain to true. -->
				<entry key="max_depth" value="10" />
				
				<!-- max_pages - how many pages to download at all per run.
					 If you set this higher then the value of WC_SEARCH_LIMIT in SNC_Runtim_Configuration 
					 (currently 10000) then that value will be used. This Runtime configuration acts as a 
					 safety margin to not exceed a certain maximum number of pages to download per run. You 
					 can, however, also set this to -1 in which case there will be no limit on the number 
					 of downloaded pages. Be careful though, as this may lead to a huge amount of pages. 
					 As a last safeguard in case this value is set to -1, this will also implicitly set the 
					 parameter of stayOnDomain to true.As a rule of thumb: If you want to check for all your 
					 keywords on one big news site, set this to -1. If you just want to get some sites off 
					 of a bulletin board, put something around 2000 in-->
				<entry key="max_pages" value="-1" />
				
				<!-- stayOnDomain - whether or not the crawler is allowed to follow links off of the 
					 initial domain 
					 If set to true, then the crawler is NOT allowed to fetch pages from a new domain.
					 It is generally not recommended to set this to false (thus allowing to leave the 
					 domain) AND have a high value in max_depth and max_pages, as this could lead to 
					 a lot of pages being downloaded from any place on the net just by following some 
					 nice linklist one has setup on his/her page  -->
				<entry key="stayOnDomain" value = "true" /> 
				
				<!-- stayBelowGivenPath - Shall the crawler ONLY get pages below the initially given path
					 This option is like an even stricter stayOnDomain. Not only we can't leave the
					 initial given domain, but any link encountered must point to some page on or below
					 the above given url. 
					 That also means, if you set this to true, you do not need to set stayOnDomain, or 
					 rather the value of stayOnDomain is ignored.
					 
					 Note: if you did not give path as part of the url, then / as the path starting point 
					 is used -->
				<entry key="stayBelowGivenPath" value = "false" /> 
								
				<!-- getOnlyRelevantPages - Get only pages containing any of the track terms -->
				<entry key="getOnlyRelevantPages" value = "true" />
				
				<!-- wordDistanceCutoffMargin - Number of words before and after track term to cut out.
					 How many words shall be between the found word (as searched by via the trackTerms 
					 from CrawlerConfiguration) and the start and end of cut out text. This option applies 
					 ONLY to the SimpleWebParser. Other text parser, such as SimpleDiscussionParser or+
					 specific ones, that use jericho to extract specific parts of a page, don't use it. -->
				<entry key="wordDistanceCutoffMargin" value = "30" />
				
				<!-- useAllCrawlerConstraints - Shall the crawler incorporate constraints from the ALL 
					 section (true), or only the ones from it's own section (false) -->
				<entry key="useAllCrawlerConstraints" value = "false" /> 
			</map>
		</property>
	</bean>
	<bean id="SimpleWebCrawlerTrigger"
		class="org.springframework.scheduling.quartz.SimpleTriggerFactoryBean">
		<property name="jobDetail" ref="SimpleWebCrawlerDetail" />
		<property name="startDelay" value="3000" />
		<property name="repeatInterval" value="86400000" /> <!-- 24 Stunden -->
	</bean>
	
	
	
	
	
	<!-- T H E   W E B   C R A W L E R 
		 THEWebCrawler is an implementation of the open source web crawler crawler4j (see
		 https://code.google.com/p/crawler4j/). 
		 It has a multi threaded software design which crawls, fetches and parses sites in parallel.
		 Because of this software design it is ightning fast and can take an arbitrary number of initial
		 urls to start the crawl process. Because of this, the only option THEWebCrawler needs to get from 
		 here, is where it's configuration file comes from. The configuration file can either be a seperate
		 file, or a section within the general crawler configuration file. In the former case, state the 
		 name of configuration file here, in the latter case enter exactly ___CRAWLER_CONFIGURATION___ 
		 
		 ATTENTION: As of this moment, Release 0.9 it is NOT possible to pass a specific configuration file
		 			here. So you must stick with the entry ___CRAWLER_CONFIGURATION___ and put all your
		 			web crawler specific settings in the standard configuration file provided by the 
		 			configuration manager below.  
		 -->
	<bean name="THEWebCrawlerDetail"
		class="org.springframework.scheduling.quartz.JobDetailFactoryBean">
		<property name="jobClass" value="de.comlineag.snc.controller.THEWebCrawlerController" />
		<property name="durability" value="true" />
		<property name="jobDataAsMap">
			<map>
				<!-- THIS MUST BE SET ACCORDING TO AN EXISTING CODE FOR A SOCIAL NETWORK ENTRY IN SocialNetworkDefinitions.xml -->
				<entry key="SN_ID_CODE" value="WO" />
				<!-- TODO move authentication parameter to the crawler configuration -->
				<!-- if the website requires authentication, set username and password here -->
				<entry key="user" value="Y29taW5lMjAxNA==" />		<!-- comine2014 -->
				<entry key="passwd" value="TUd1LTZ0Yy1BUjUtdTdS" /> <!-- MGu-6tc-AR5-u7R -->
				
				<!-- TODO make passing a specific crawler configuration file possible -->
				<!-- <entry key="configDBHandler" value="CortalConsorsSpecificWebCrawlerConfiguration.xml" /> -->
				<entry key="configDbHandler" value="___CRAWLER_CONFIGURATION___" />
			</map>
		</property>
	</bean>
	<bean id="THEWebCrawlerTrigger"
		class="org.springframework.scheduling.quartz.SimpleTriggerFactoryBean">
		<property name="jobDetail" ref="THEWebCrawlerDetail" />
		<property name="startDelay" value="3000" />
		<!-- <property name="repeatInterval" value="86400000" />  --><!-- 24 Stunden -->
		<property name="repeatInterval" value="60000" /> 
	</bean>
	
	
	
	
	
	<!-- F A C E B O O K    C R A W L E R    (TO BE IMPLEMENTED) -->
	<bean name="FacebookCrawlerDetail"
		class="org.springframework.scheduling.quartz.JobDetailFactoryBean">
		<property name="jobClass" value="de.comlineag.snc.crawler.Facebook4JFbCrawler" />
		<!-- <property name="concurrent" value="false" /> -->
		<property name="durability" value="true" />
		<property name="jobDataAsMap">
			<map>
				<!-- THIS MUST BE SET ACCORDING TO AN EXISTING CODE FOR A SOCIAL NETWORK ENTRY IN SNC_Runtime_Configuration.xml -->
				<entry key="SN_ID_CODE" value="FB" />
				
				<!-- These are security tokens -->
				<entry key="appId" value="1375488886009208" />
				<entry key="appSecret" value="1d8ff81333b5dc9ba6ef0de228dfb130" />
				<entry key="accessToken" value="1375488886009208|1d8ff81333b5dc9ba6ef0de228dfb130" />
				<entry key="permissionSet" value="" />
				
				<!-- with these entries the url to the rest api and graph engine is generated -->
				<entry key="protocol" value="https" /> 					<!-- the access protocol -->
				<entry key="port" value="443" /> 						<!-- the port to use -->
				<entry key="server_url" value="graph.facebook.com" /> 	<!-- the actual host -->
				<entry key="location" value="" /> 						<!-- an addition to the host part of the location -->
			</map>
		</property>
	</bean>
	<bean id="FacebookCrawlerTrigger"
		class="org.springframework.scheduling.quartz.SimpleTriggerFactoryBean">
		<property name="jobDetail" ref="FacebookCrawlerDetail" />
		<!-- 1 second delay after startup -->
		<property name="startDelay" value="1000" />
		<!-- repeat the job every 
							5000 - 50 seconds 
							6000 - 1 minutes 
							15000 - 2,5 minutes (round about) 
							30000 - 5 minutes -->
		<property name="repeatInterval" value="30000" />
	</bean>
	
	
	
	
	
	<!-- G O O G L E    C R A W L E R    (TO BE DEFINED) -->
<!--  
	<bean name="GoogleCrawlerDetail" class="org.springframework.scheduling.quartz.JobDetailFactoryBean"> 
		<property name="jobClass" value="de.comlineag.snc.crawler.GoogleCrawler" /> 
		<property name="concurrent" value="false" />
		<property name="durability" value="true" />
		<property name="jobDataAsMap">
			<map> 
				<entry key="SN_ID_CODE" value="GP" />
				
				<entry key="consumerKey" value="" /> 
				<entry key="consumerSecret" value="" /> 
				<entry key="token" value="" /> 
				<entry key="tokenSecret" value="" /> 
				
				<entry key="user" value="" /> 
				<entry key="passwd" value="" /> 
				
				<entry key="protocol" value="https" /> 
				<entry key="port" value="443" /> 
				<entry key="server_url"	value="google.com" /> 
				<entry key="graph_api_loc" value="/graph" /> 
				<entry key="rest_api_loc" value="/restapi/" /> 
			</map> 
		</property> 
	</bean> 
	<bean id="GoogleCrawlerTrigger" class="org.springframework.scheduling.quartz.SimpleTriggerFactoryBean"> 
		<property name="jobDetail" ref="GoogleCrawlerDetail" />
		<property name="startDelay" value="1000" />
		<property name="repeatInterval" value="30000" /> 
	</bean> 
-->
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
	     END OF CRAWLER SECTION - END OF CRAWLER SECTION - END OF CRAWLER SECTION - EN 
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	
	
	
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
	     C O N F I G U R A T I O N    S E T T I N G S  -  C O N F I G U R A T I O N  S    
	     You may choose exactly one of the provided configuration manager 
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	
	<!-- this is the bean to get the crawler configuration from the HANA (TO BE IMPLEMENTED) -->
	<!-- 
	<bean id="configurationManager" class="de.comlineag.snc.persistence.HANAConfigurationPersistence"> 
		<property name="protocol" value="http" /> 
		<property name="host" value="192.168.131.30" /> 
		<property name="port" value="8000" /> 
		<property name="jdbcPort" value="30015" /> 
		
		<property name="location" value="/comline/snc/services" /> 
		<property name="user" value="U0JNX0VYVA==" /> 
		<property name="pass" value="TW9uIzRwb3Mhc29DSUFM" /> 
	</bean> 
	-->

	<!-- this is the bean to get the crawler configuration specific for individual customers AND/OR domains from an xml file
		(WORKS AND PROVIDES THE ABILITY TO SETUP DIFFERENT CRAWLER CONFIGURATIONS FOR EITHER CUSTOMER OR DOMAIN)
 		If you change file name or location you can either put an absolute path in here, or a relative one. If
 		you enter a relative path, this HAS to be below WEB-INF! This is because the runtime configuration checks 
 		for an absolute path and if it does not find one (no / // or drive letter like c: at the start), it 
 		assumes the file to be located in or below the WEB-INF directory -->
	<bean id="configurationManager" class="de.comlineag.snc.persistence.ComplexXmlConfigurationPersistence"> 
		<property name="configDbHandler" value="SNC_Crawler_Configuration/CortalConsorsSpecificCrawlerConfiguration.xml" />
	</bean> 
	
	<!-- this is the bean to get the crawler configuration from a simple xml file 
		(WORKS BUT NO CUSTOMER SPECIFIC CONFIGURATION POSSIBLE)	
 		If you change file name or location you can either put an absolute path in here, or a relative one. If
 		you enter a relative path, this HAS to be below WEB-INF! This is because the runtime configuration checks 
 		for an absolute path and if it does not find one (no / // or drive letter like c: at the start), it 
 		assumes the file to be located in or below the WEB-INF directory -->
<!-- 
	<bean id="configurationManager" class="de.comlineag.snc.persistence.SimpleXmlConfigurationPersistence"> 
		<property name="configDbHandler" value="webapp/WEB-INF/SNC_Crawler_Configuration/SimpleCrawlerConfiguration.xml" /> 
	</bean>
-->
	 
	<!-- this is the bean to get the crawler configuration from an ini file 
		(WORKS WITH RESTRICTIONS - NO CUSTOMER AND NO CRAWLER SPECIFIC CONFIGURATION POSSIBLE) 
 		If you change file name or location you can either put an absolute path in here, or a relative one. If
 		you enter a relative path, this HAS to be below WEB-INF! This is because the runtime configuration checks 
 		for an absolute path and if it does not find one (no / // or drive letter like c: at the start), it 
 		assumes the file to be located in or below the WEB-INF directory -->
<!-- 
	<bean id="configurationManager" class="de.comlineag.snc.persistence.IniFileConfigurationPersistence">
		<property name="configDbHandler" value="webapp/WEB-INF/SNC_Crawler_Configuration/CrawlerConfiguration.ini" />
	</bean>
 -->
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
	     END OF CONFIGURATION SECTION - END OF CONFIGURATION SECTION - END OF CONFIGUR 
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	
	
	
	
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
	     P E R S I S T E N C E    S E T T I N G S  -  P E R S I S T E N C E    S E T T
	     You may choose exactly one of the provided persistence manager 
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	<!-- N U L L    P E R S I S T E N C E    A K A    D I S C A R D    D A T A    (WORKS)  
	     This "persistence manager" sends all data into the void. Nothing is checked,
	     nothing is saved. It is therefore of limited use, from a persistence point of
	     view, but on the other hand the fastest available persistence manager.
	     In case, you want to use a graph database as the only persistence layer, you
	     activate the NullPersistence Manager here and choose your preferred graph db 
	     engine below. 
	     You also have to set activateGraphDb in SNC_Runtime_Configuration-<version>.xml 
	     section runtime to true. -->
	<!-- -->
	<bean id="persistenceManager" class="de.comlineag.snc.persistence.NullPersistence" /> 
	
	
	<!-- F I L E S Y S T E M    J S O N    P E R S I S T E N C E    (WORKS)  
	     This one simply stores all posts and users as json files on disk. It is mainly 
	     thought of as a development and not as a runtime persistence manager. It uses 
	     the same path options as the FailSavePersistence and as such does not need any 
	     options. All options, like the path to the directory, is taken from 
	     SNC_Runtime_Configuration-<VERSION>.xml.
	     If you activate this storage manager, you should DEFINATELY turn off the FsCrawler 
	     in the Job control section, otherwise your system will end up storing and restoring
	     the same objects as json files with no end in sight -->
	<!-- 
	<bean id="persistenceManager" class="de.comlineag.snc.persistence.JsonFilePersistence" /> 
	-->
	
	<!-- H A N A    P E R S I S T E N C E    (WORKS) 
		 The SAP HANA InMemory DB persistence can be used via JDBC or OData. As such,
		 both options are set in this section. The properties for user and password
		 need to be crypted in the format as specified in the crypto section for
		 configuration options, called Configuration Crypto Settings. That means, if
		 you used Base64 to encode your login options for the db, activate the Base64 
		 crypto provider below.
		 
		 If you activate the HANA persistence manager, make sure to also activate 
		 the FsCrawler in the job control section. In case HANA is unable to store your 
		 data or the crawler is unable to connect to HANA, a FailSavePersistcenManager 
		 is activated, which stores all failed objects as json file on disk. The FsCrawler 
		 is used to scan the storage directory for failed objects and then tries to 
		 insert them in the HANA DB. -->
	<!-- 
	<bean id="persistenceManager" class="de.comlineag.snc.persistence.HANAPersistence">
		<property name="host" value="192.168.131.30" />
		<property name="port" value="8000" />
		<property name="jdbcPort" value="30015" />
		<property name="protocol" value="http" />
		<property name="dbDriver" value="com.sap.db.jdbc.Driver" />
		<property name="location" value="/comline/saa/services" />
		<property name="serviceUserEndpoint" value="saveUser.xsodata" />
		<property name="servicePostEndpoint" value="savePost.xsodata" />
		<property name="user" value="U0JNX0VYVA==" />
		<property name="pass" value="TW9uIzRwb3Mhc29DSUFM" />
	</bean>
	 -->
	 
	<!-- C O U C H D B    P E R S I S T E N C E    (in progress) -->
	<!-- 
	<bean id="persistenceManager" class="de.comlineag.snc.persistence.CouchDbPersistence">
		<property name="host" value="localhost" />
		<property name="port" value="8000" />
		<property name="protocol" value="http" />
		<property name="location" value="/comline/saa/services" />
		<property name="serviceUserEndpoint" value="saveUser.xsodata" />
		<property name="servicePostEndpoint" value="savePost.xsodata" />
		<property name="user" value="admin" />
		<property name="pass" value="admin1234" />
	</bean>
	 -->
	
	<!-- R I A K - D B    P E R S I S T E N C E    (in progress) -->
	<!-- 
	<bean id="persistenceManager" class="de.comlineag.snc.persistence.RiakDbPersistence">
		<property name="host" value="localhost" />
		<property name="port" value="8000" />
		<property name="protocol" value="http" />
		<property name="location" value="/comline/saa/services" />
		<property name="serviceUserEndpoint" value="saveUser.xsodata" />
		<property name="servicePostEndpoint" value="savePost.xsodata" />
		<property name="user" value="admin" />
		<property name="pass" value="admin1234" />
	</bean>
	-->
	 
	<!-- M A P D B    P E R S I S T E N C E    (TO BE IMPLEMENTED) -->
	<!-- 
	<bean id="persistenceManager" class="de.comlineag.snc.persistence.MapDBPersistence"> 
		<property name="protocol" value="t.b.d." /> 
		<property name="host" value="localhost"	/> 
		<property name="port" value="t.b.d." /> 
		<property name="location" value="/db/data" /> 
		<property name="db_path" value="/db/data" /> 
	</bean> 
	-->
	
	
	<!-- G R A P H    E N G I N E     P E R S I S T E N C E    M A N A G E R 
		 this defines the graph db system to use. The general usage of a graph db system
		 is activated in the  SNC_Runtime_Configuration-<VERSION>.xml with the parameter 
		 activateGraphDb. In here you merely choose which graph db system to use. 
		 As of the moment only Neo4J is available. -->
	<!-- N E O 4 J    P E R S I S T E N C E    (IN DEVELOPMENT) -->
	<bean id="graphPersistenceManager" class="de.comlineag.snc.persistence.Neo4JPersistence"/>
	
	<!-- N E O 4 J    E M B E D D E D    P E R S I S T E N C E    (IN DEVELOPMENT) -->
	<!-- <bean id="graphPersistenceManager" class="de.comlineag.snc.persistence.Neo4JEmbeddedPersistence" /> --> 
	
	<!-- H A N A    P E R S I S T E N C E    (DOES NOT WORK - NO GRAPH SUPPORT IN HANA YET)  -->
	<!-- 
	<bean id="graphPersistenceManager" class="de.comlineag.snc.persistence.HANAGraphPersistence">
		<property name="host" value="192.168.131.30" />
		<property name="port" value="8000" />
		<property name="protocol" value="http" />
		<property name="location" value="/graph/comline/saa/services" />
		<property name="user" value="U0JNX0VYVA==" />
		<property name="pass" value="TW9uIzRwb3Mhc29DSUFM" />
	</bean>
	-->
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
	     END OF PERSISTENCE SECTION - END OF PERSISTENCE SECTION - END OF PERSISTENCE  
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	
	
	
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
	     C R Y P T O    S E T T I N G S  -  C R Y P T O    S E T T I N G S  -  C R Y P   
	     You can choose exactly one of the available crypto provider for each category (id) 
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	
	<!-- C O N F I G U R A T I O N     C R Y P T O     S E T T I N G S 
		 used to encrypt configuration settings for the persistence connection --> 
	<!-- NULL   Crypto Provider   -   this is the bean to set the configuration encryption provider to null -->
	<!-- <bean id="configurationCryptoProvider" class="de.comlineag.snc.crypto.NullCryptoProvider"> </bean> -->
	
	<!-- BASE64 Crypto Provider   -   this is the bean to set the configuration encryption provider to Base64 -->
	<bean id="configurationCryptoProvider" class="de.comlineag.snc.crypto.Base64CryptoProvider"> </bean>
	
	<!-- DES    Crypto Provider   -   this is the bean to set the configuration encryption provider to DES -->
	<!-- <bean id="configurationCryptoProvider" class="de.comlineag.snc.crypto.DesCryptoProvider"> </bean> -->
	
	<!-- 3DES   Crypto Provider   -   this is the bean to set the configuration encryption provider to Triple DES -->
	<!-- <bean id="configurationCryptoProvider" class="de.comlineag.snc.crypto.Des3CryptoProvider"> </bean> -->
	
	<!-- AES    Crypto Provider   -   this is the bean to set the configuration encryption provider to AES -->
	<!-- <bean id="configurationCryptoProvider" class="de.comlineag.snc.crypto.AesCryptoProvider"> </bean> -->
	
	
	<!-- D A T A     C R Y P T O     S E T T I N G S
		 used to encrypt/decrypt the data within the persistence (DB) - has nothing to do with the configuration -->
	<!-- NULL   Crypto Provider   -   this is the bean to set the data encryption provider to null - recommended -->
	<bean id="dataCryptoProvider" class="de.comlineag.snc.crypto.NullCryptoProvider"> </bean>
	
	<!-- BASE64 Crypto Provider   -   this is the bean to set the data encryption provider to Base64 -->
	<!-- <bean id="dataCryptoProvider" class="de.comlineag.snc.crypto.Base64CryptoProvider"> </bean> -->
	
	<!-- DES    Crypto Provider   -   this is the bean to set the data encryption provider to DES -->
	<!-- <bean id="dataCryptoProvider" class="de.comlineag.snc.crypto.DesCryptoProvider"> 
			<property name="initialVector"dfhjhgcfngcfhcecf4tcdgc5vtrgvhtdzvcgcdbzvt5cxhvxtycsvtsctyvzs5zcc5zzvhcdht></property>
		</bean> 
	-->
	
	<!-- 3DES   Crypto Provider   -   this is the bean to set the data encryption provider to Triple DES -->
	<!-- <bean id="dataCryptoProvider" class="de.comlineag.snc.crypto.Des3CryptoProvider"> </bean> -->
	
	<!-- AES    Crypto Provider   -   this is the bean to set the data encryption provider to AES -->
	<!-- <bean id="dataCryptoProvider" class="de.comlineag.snc.crypto.AesCryptoProvider"> </bean> -->
	
	
	<!-- S E A R C H     C R Y P T O     S E T T I N G S
		 used to encrypt/decrypt the search constraints from the crawler configuration -->
	<!-- NULL   Crypto Provider   -   this is the bean to set the search encryption provider to null -->
	<bean id="searchCryptoProvider" class="de.comlineag.snc.crypto.NullCryptoProvider"> </bean>
	
	<!-- BASE64 Crypto Provider   -   this is the bean to set the data encryption provider to Base64 --> 
	<!-- <bean id="searchCryptoProvider" class="de.comlineag.snc.crypto.Base64CryptoProvider"> </bean> -->
	
	<!-- DES    Crypto Provider   -   this is the bean to set the data encryption provider to DES -->
	<!-- <bean id="searchCryptoProvider" class="de.comlineag.snc.crypto.DesCryptoProvider"> 
			<property name="initialVector"dfhjhgcfngcfhcecf4tcdgc5vtrgvhtdzvcgcdbzvt5cxhvxtycsvtsctyvzs5zcc5zzvhcdht></property>
		</bean>
	-->
	
	<!-- 3DES   Crypto Provider   -   this is the bean to set the data encryption provider to Triple DES -->
	<!-- <bean id="searchCryptoProvider" class="de.comlineag.snc.crypto.Des3CryptoProvider"> </bean> -->
	
	<!-- AES    Crypto Provider   -   this is the bean to set the data encryption provider to AES -->
	<!-- <bean id="searchCryptoProvider" class="de.comlineag.snc.crypto.AesCryptoProvider"> </bean> -->
	
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
	     END OF CRYPTO SECTION - END OF CRYPTO SECTION - END OF CRYPTO SECTION - END O
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	
	
	
	<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
	     NO NEED TO CHANGE ANYTHING BELOW THIS LINE - IF YOU DO CHANGE ANYTHING BE VERY
	     CAREFUL WHAT YOU DO HERE - IF YOU CHANGE ANYTHING IT MIGHT KILL THE CONNECTOR! 
	     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
	<!-- F S    C R A W L E R    (IN DEVELOPMENT)		-->
	<bean name="FsCrawlerDetail"
		class="org.springframework.scheduling.quartz.JobDetailFactoryBean">
		<property name="jobClass" value="de.comlineag.snc.crawler.FsCrawler" />
		<!-- <property name="concurrent" value="false" /> -->
		<property name="durability" value="true" />
		<property name="jobDataAsMap">
			<map>
				<entry key="fileNamePattern" value=".*_fail.json" />
				<!-- make absolutely sure, that these entries are in sync with SNC_Runtime_Configuration.xml -->
				<entry key="StoragePath" value="storage"/>
				<entry key="JsonBackupStoragePath" value="json"/>
				<entry key="ProcessedJsonBackupStoragePath" value="processedJson"/>
				<entry key="InvalidJsonBackupStoragePath" value="invalidJson"/>
				<entry key="MoveOrDeleteProcessedJsonFiles" value="move"/>
			</map>
		</property>
	</bean>
	<bean id="FsCrawlerTrigger"
		class="org.springframework.scheduling.quartz.SimpleTriggerFactoryBean">
		<property name="jobDetail" ref="FsCrawlerDetail" />
		<!-- <property name="startDelay" value="30000" /> -->
		<property name="startDelay" value="300" />
		<property name="repeatInterval" value="180000" />
	</bean>
	
	<!-- Quartz scheduler properties 
		 these entries are needed for the JMX Access to the Job Factory, 
		 so that the job state can be monitored by the administration website -->
	<bean id="quartzScheduler" class="org.springframework.scheduling.quartz.SchedulerFactoryBean">
		<property name="quartzProperties"> 
			<util:properties>
				<!-- Must be set to true, otherwise the Quartz scheduler is not registered in the JMX server. -->
				<prop key="org.quartz.scheduler.jmx.export">true</prop>
				<!-- JMX object name the Quartz scheduler is registered under in the JMX server. -->
				<prop key="org.quartz.scheduler.jmx.objectName"> quartz:type=QuartzScheduler,name=SNC</prop>
			</util:properties>
  		</property>
	</bean>
</beans>
